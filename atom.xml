<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[NLPNote]]></title>
  <link href="http://hdqzzz.github.io/atom.xml" rel="self"/>
  <link href="http://hdqzzz.github.io/"/>
  <updated>2015-02-06T12:39:29+08:00</updated>
  <id>http://hdqzzz.github.io/</id>
  <author>
    <name><![CDATA[Joey Zhao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Locality Sensitive Hash]]></title>
    <link href="http://hdqzzz.github.io/blog/2015/02/06/introduction-to-locality-sensitive-hash/"/>
    <updated>2015-02-06T11:35:32+08:00</updated>
    <id>http://hdqzzz.github.io/blog/2015/02/06/introduction-to-locality-sensitive-hash</id>
    <content type="html"><![CDATA[<p>Locality-sensitive hashing (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). The hashing used in LSH is different from conventional hash functions, such as those used in cryptography, as in the LSH case the goal is to maximize probability of &ldquo;collision&rdquo; of similar items rather than avoid collisions. [1] Note how locality-sensitive hashing, in many ways, mirrors data clustering and nearest neighbor search.</p>

<h1>Title1</h1>

<h2>Title2</h2>

<h3>Title3</h3>

<p>An LSH family [1] [2] [3] \mathcal F is defined for a metric space \mathcal M =(M, d), a threshold R>0 and an approximation factor c>1. This family \mathcal F is a family of functions h:{\mathcal M}\to S which map elements from the metric space to a bucket s \in S. The LSH family satisfies the following conditions for any two points p, q \in {\mathcal M}, using a function h \in \mathcal F which is chosen uniformly at random:</p>

<p>if d(p,q) \le R, then h(p)=h(q) (i.e.,p and q collide) with probability at least P_1,
   if d(p,q) \ge cR, then h(p)=h(q) with probability at most P_2.
   A family is interesting when P_1>P_2. Such a family \mathcal F is called (R,cR,P_1,P_2)-sensitive.</p>

<p>   Alternatively[4] it is defined with respect to a universe of items U that have a similarity function \phi : U \times U \to [0,1]. An LSH scheme is a family of hash functions H coupled with a probability distribution D over the functions such that a function h \in H chosen according to D satisfies the property that Pr_{h \in H} [h(a) = h(b)] = \phi(a,b) for any a,b \in U.</p>

<p>   Amplification[edit]
   Given a (d_1, d_2, p_1, p_2)-sensitive family \mathcal F, we can construct new families \mathcal G by either the AND-construction or OR-construction of \mathcal F.[1]</p>

<p>   To create an AND-construction, we define a new family \mathcal G of hash functions g, where each function g is constructed from k random functions h_1, &hellip;, h_k from \mathcal F. We then say that for a hash function g \in \mathcal G, g(x) = g(y) if and only if all h_i(x) = h_i(y) for i = 1, 2, &hellip;, k. Since the members of \mathcal F are independently chosen for any g \in \mathcal G, \mathcal G is a (d_1, d_2, p<em>{1}^k, p</em>{2}^k)-sensitive family.</p>

<p>   To create an OR-construction, we define a new family \mathcal G of hash functions g, where each function g is constructed from k random functions h_1, &hellip;, h_k from \mathcal F. We then say that for a hash function g \in \mathcal G, g(x) = g(y) if and only if h_i(x) = h_i(y) for one or more values of i. Since the members of \mathcal F are independently chosen for any g \in \mathcal G, \mathcal G is a (d_1, d_2, 1- (1 - p_1)<sup>k</sup>, 1 - (1 - p_2)<sup>k</sup>)-sensitive family.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World]]></title>
    <link href="http://hdqzzz.github.io/blog/2015/01/06/hello-world/"/>
    <updated>2015-01-06T22:29:40+08:00</updated>
    <id>http://hdqzzz.github.io/blog/2015/01/06/hello-world</id>
    <content type="html"><![CDATA[<h3>NLP的主要应用范畴</h3>

<p>文本朗读（Text to speech）/语音合成（Speech synthesis）
语音识别（Speech recognition）
中文自动分词（Chinese word segmentation）
词性标注（Part-of-speech tagging）
句法分析（Parsing）
自然语言生成（Natural language generation）
文本分类（Text categorization）
信息检索（Information retrieval）
信息抽取（Information extraction）
文字校对（Text-proofing）
问答系统（Question answering）
机器翻译（Machine translation）
自动摘要（Automatic summarization）
文字蕴涵（Textual entailment）</p>

<h3>NLP的研究难点</h3>

<p>单词的边界界定
在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。
词义的消歧
许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。
句法的模糊性
自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。
有瑕疵的或不规范的输入
例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字符识别（OCR）的错误。
语言行为与计划
句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。</p>
]]></content>
  </entry>
  
</feed>
